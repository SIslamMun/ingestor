# Research Report: Lightweight Fine-Tuned LLMs with Reasoning Capabilities

### Key Points
*   **DeepSeek-R1 Distill Models** (1.5B–32B) currently represent the state-of-the-art in open-weight reasoning for small language models (SLMs). By distilling the "Chain of Thought" (CoT) patterns from the massive 671B DeepSeek-R1 model, the 1.5B variant reportedly outperforms GPT-4o on specific math benchmarks like AIME and MATH [cite: 1, 2, 3].
*   **Microsoft Phi-4-mini** (3.8B) and its specialized variant **Phi-4-mini-reasoning** utilize a "reasoning dense" synthetic data curriculum. These models are engineered for memory-constrained environments, achieving performance parity with much larger models in STEM tasks through a focus on data quality rather than parameter scale [cite: 4, 5, 6].
*   **Qwen 3** (0.6B) introduces a native "Thinking Mode" in sub-billion parameter models. This allows for dynamic switching between rapid, general-purpose generation and deep, multi-step reasoning, making it a pioneer in bringing complex logic to extreme edge devices [cite: 7, 8].
*   **Gemma 3** (270M) and **SmolLM3** (3B) demonstrate the viability of "dual-mode" reasoning and on-device deployment. Gemma 3 270M is specifically optimized for task-specific fine-tuning on mobile hardware, while SmolLM3 offers a "think/no_think" toggle for balancing latency and logical depth [cite: 9, 10].

### Executive Summary
The landscape of Large Language Models (LLMs) has shifted dramatically in 2024 and 2025, moving away from a sole focus on massive parameter counts toward the optimization of **Small Language Models (SLMs)**—typically defined as models under 7 billion parameters. The primary driver of this shift is the successful induction of **reasoning capabilities**—previously thought to be an emergent property of scale—into lightweight architectures. This has been achieved through three primary methodologies: **Knowledge Distillation** from reasoning-specialized teacher models (e.g., DeepSeek-R1), **Synthetic Data Curricula** focused on step-by-step logic (e.g., Phi-4), and **Reinforcement Learning** techniques like Group Relative Policy Optimization (GRPO) applied to smaller bases (e.g., Gemma 3).

This report details the technical specifications, training methodologies, and benchmark performance of the leading lightweight reasoning models available as of early 2025. It analyzes how models like DeepSeek-R1-Distill-Qwen-1.5B, Phi-4-mini, and Qwen 3 0.6B are reshaping edge AI by enabling complex mathematical and logical inference on consumer-grade hardware.

---

## 1. DeepSeek-R1 Distill Series
The DeepSeek-R1 Distill series represents a paradigm shift in how reasoning capabilities are instantiated in small models. Rather than training small models from scratch on raw web data, DeepSeek utilizes a "teacher-student" distillation framework where the reasoning patterns of the massive DeepSeek-R1 (671B) are transferred to smaller dense architectures.

### 1.1 Architecture and Distillation Methodology
The DeepSeek-R1 Distill models are not trained using Reinforcement Learning (RL) from scratch, unlike their teacher model. Instead, they are **fine-tuned** on a curated dataset of reasoning trajectories generated by DeepSeek-R1 [cite: 1, 2].
*   **Base Architectures**: The series utilizes two primary base architectures: **Qwen-2.5** and **Llama-3.1**.
*   **Distillation Process**: DeepSeek-R1 (the teacher) generates vast amounts of Chain-of-Thought (CoT) data, including intermediate reasoning steps enclosed in `<think>` tags. This data is filtered for correctness and readability before being used to Supervised Fine-Tune (SFT) the smaller student models [cite: 1, 11].
*   **Result**: The student models inherit the "thinking" behavior—pausing to generate an internal monologue of logic—before producing a final answer. This allows a 1.5B model to mimic the problem-solving depth of a 671B model, albeit with limited world knowledge [cite: 3].

### 1.2 Model Variants and Specifications
DeepSeek has released a suite of distilled models to cater to different compute constraints:

| Model Name | Base Architecture | Parameters | Context Window | Key Capabilities |
| :--- | :--- | :--- | :--- | :--- |
| **DeepSeek-R1-Distill-Qwen-1.5B** | Qwen-2.5 | 1.5B | 128k | Outperforms GPT-4o on AIME/MATH; runs on edge devices [cite: 3, 12]. |
| **DeepSeek-R1-Distill-Qwen-7B** | Qwen-2.5 | 7B | 128k | Balanced performance; beats larger open-source models on code/math [cite: 1, 11]. |
| **DeepSeek-R1-Distill-Llama-8B** | Llama-3.1 | 8B | 128k | Strong mathematical reasoning; leverages Llama ecosystem compatibility [cite: 12]. |
| **DeepSeek-R1-Distill-Qwen-14B** | Qwen-2.5 | 14B | 128k | High reasoning accuracy; approaches frontier model performance [cite: 12]. |
| **DeepSeek-R1-Distill-Qwen-32B** | Qwen-2.5 | 32B | 128k | State-of-the-art for dense models; outperforms OpenAI-o1-mini [cite: 1, 2]. |

### 1.3 Performance and Benchmarks
The performance of the 1.5B model is particularly notable. Research indicates that **DeepSeek-R1-Distill-Qwen-1.5B** achieves superior results on the **AIME** (American Invitational Mathematics Examination) and **MATH** benchmarks compared to GPT-4o and Claude 3.5 Sonnet [cite: 3, 12].
*   **Math & Logic**: The 1.5B model excels at structured problem solving where the answer can be derived via strict logic.
*   **Coding**: While strong in logic, the 1.5B model struggles with complex coding tasks (scoring 16.9 on LiveCodeBench) compared to the 32B variant or larger models [cite: 12].
*   **Efficiency**: The 1.5B model can be quantized (e.g., Q4_KM) to run on devices with less than 2GB of RAM, making it viable for mobile deployment [cite: 13].

---

## 2. Microsoft Phi-4 Family
Microsoft's Phi series has consistently pushed the boundaries of SLMs by prioritizing "textbook-quality" data. The Phi-4 generation introduces specialized reasoning models that leverage synthetic data to punch significantly above their weight class.

### 2.1 Phi-4-mini and Phi-4-mini-reasoning
**Phi-4-mini** is a 3.8 billion parameter model designed to be a general-purpose workhorse for edge computing.
*   **Parameters**: 3.8B.
*   **Context Length**: 128k tokens [cite: 14].
*   **Training Data**: 5 trillion tokens, comprising filtered web data and high-quality synthetic data focused on reasoning [cite: 14, 15].

**Phi-4-mini-reasoning** is a specialized variant fine-tuned specifically for math and logic.
*   **Training Methodology**: It is fine-tuned on **synthetic mathematical content** generated by advanced reasoning models (like DeepSeek-R1). The dataset includes over one million diverse math problems with multiple solution paths (rollouts), filtered for correctness [cite: 5].
*   **Capabilities**: Designed for formal proof generation, symbolic computation, and advanced word problems. It excels at maintaining context across multi-step logic chains [cite: 5, 16].

### 2.2 Phi-4-mini-flash-reasoning
A unique entry in the family is **Phi-4-mini-flash-reasoning**, which utilizes a hybrid architecture.
*   **Architecture**: **SambaY** hybrid architecture. It combines **Mamba State-Space Models (SSMs)** for linear-time context prefilling with **Sliding Window Attention** and **Gated Memory Units (GMUs)**.
*   **Latency**: This architecture allows for ultra-low latency inference, making it ideal for real-time reasoning on edge devices [cite: 4, 17].
*   **Use Case**: Optimized for scenarios requiring rapid multi-step reasoning where standard Transformer latency would be prohibitive [cite: 17].

### 2.3 Synthetic Data Strategy
The core of Phi-4's success is its data recipe. Unlike models trained primarily on organic web data, Phi-4 uses synthetic data to create a "curriculum" of reasoning.
*   **Pivotal Token Search**: Microsoft employs techniques to identify "pivotal tokens" in reasoning chains—steps where the model is most likely to make an error—and optimizes the training data to reinforce correct paths at these critical junctures [cite: 18, 19].
*   **Performance**: Phi-4-mini matches the performance of models twice its size on math and coding benchmarks [cite: 20, 21].

---

## 3. Qwen 3 Series
Alibaba Cloud's Qwen 3 series introduces a novel "Thinking Mode" directly into the model architecture, allowing for a unified model that can handle both casual chat and intense reasoning.

### 3.1 Qwen3-0.6B: The Micro-Reasoning Giant
**Qwen3-0.6B** is one of the smallest capable reasoning models available, with only 600 million parameters (0.44B non-embedding parameters) [cite: 8, 22].
*   **Hybrid Thinking Mode**: The model supports two distinct modes:
    1.  **Thinking Mode**: Activated via the `/think` command or system prompt. The model generates a CoT enclosed in `<think>` tags before the final answer. This mode is optimized for math, coding, and logic [cite: 8, 22, 23].
    2.  **Non-Thinking Mode**: Standard generation for general dialogue and rapid responses [cite: 8].
*   **Performance**: In Thinking Mode, Qwen3-0.6B achieves **77.6 on MATH-500** and **55.6 on MMLU-Redux**, scores that are competitive with much larger models [cite: 23].

### 3.2 Architecture and Training
*   **Dense Architecture**: The 0.6B model uses a dense Transformer architecture with **Grouped Query Attention (GQA)** to optimize inference speed [cite: 8, 24].
*   **Strong-to-Weak Distillation**: Qwen 3 utilizes a distillation pipeline where larger "Frontier Models" (like Qwen3-235B) teach smaller models. This involves multiple stages:
    1.  **Long-CoT Cold Start**: Initial training on long reasoning chains.
    2.  **Reasoning RL**: Reinforcement learning to refine logic.
    3.  **Thinking Mode Fusion**: Integrating the dual-mode capability [cite: 25].
*   **Multilingual Support**: Trained on 36 trillion tokens covering 119 languages, making it one of the most linguistically diverse SLMs [cite: 24, 26].

---

## 4. Gemma 3 Family & GRPO
Google's Gemma 3 family focuses on multimodal capabilities and extreme efficiency. While the base models are strong, the community has leveraged them extensively for reasoning fine-tuning using **Group Relative Policy Optimization (GRPO)**.

### 4.1 Gemma 3 270M
**Gemma 3 270M** is a hyper-efficient model designed for task-specific fine-tuning.
*   **Parameters**: 270 Million.
*   **Efficiency**: Runs on smartphones (e.g., Pixel 9 Pro) using less than 1% battery for 25 conversations [cite: 10].
*   **Use Case**: It is intended to be a "foundation" for specialized tasks. Users are encouraged to fine-tune it for specific logical tasks (e.g., text classification, structured data extraction) rather than using it as a general-purpose chatbot [cite: 10, 27].
*   **Architecture**: High ratio of embedding parameters (170M) to transformer parameters (100M) due to a large vocabulary (256k tokens), facilitating multilingual adaptation [cite: 10].

### 4.2 GRPO Fine-Tuning for Reasoning
The research community has identified Gemma 3 (particularly the 1B and 4B variants) as an excellent candidate for **GRPO** fine-tuning to induce reasoning capabilities similar to DeepSeek-R1.
*   **GRPO Method**: Unlike PPO (Proximal Policy Optimization), which requires a critic model, GRPO estimates the "advantage" of a response by comparing it to a group of other responses generated by the same policy for the same prompt. This reduces memory usage, making it feasible to train reasoning models on consumer GPUs [cite: 28, 29, 30, 31].
*   **Implementation**: Tutorials and repositories demonstrate fine-tuning Gemma 3 on the **GSM8K** dataset using GRPO. The model is rewarded for correct answers and proper formatting (e.g., enclosing reasoning in XML tags) [cite: 29, 32, 33].
*   **Results**: Fine-tuned Gemma 3 models show significant improvements in logical consistency and self-correction behaviors [cite: 31].

---

## 5. SmolLM3
Hugging Face's **SmolLM3** is a 3B parameter model that explicitly targets the "sweet spot" between extreme compression and capability.

### 5.1 Dual-Mode Reasoning
Like Qwen 3, **SmolLM3-3B-Instruct** supports a dual-mode operation:
*   **Thinking Mode**: Triggered by specific system prompts, enabling the model to generate reasoning traces for complex queries.
*   **Standard Mode**: For chat and simple tasks.
*   **Training**: Pre-trained on 11 trillion tokens and fine-tuned on a mixture of web data, code, math, and reasoning datasets [cite: 34, 35].

### 5.2 Performance
*   **Context**: Supports up to 128k context length via YaRN scaling [cite: 35, 36].
*   **Benchmarks**: Outperforms Llama-3.2-3B and Qwen2.5-3B on reasoning and knowledge benchmarks. It rivals larger 4B models like Gemma 3 4B in specific tasks [cite: 36, 37].
*   **Deployment**: Fully compatible with ONNX and GGUF for local deployment [cite: 36].

---

## 6. Comparative Analysis

### 6.1 Benchmark Comparison (Math & Reasoning)
*Note: Scores are approximate based on reported values in research notes.*

| Model | Parameters | GSM8K | MATH / MATH-500 | AIME | Key Strength |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **DeepSeek-R1-Distill-Qwen-1.5B** | 1.5B | High | High (beats GPT-4o) | High | Pure Math/Logic [cite: 3] |
| **Phi-4-mini-reasoning** | 3.8B | ~86% | High | Competitive | STEM & Symbolic Logic [cite: 20] |
| **Qwen3-0.6B (Thinking)** | 0.6B | ~59.6% | 77.6 (MATH-500) | - | Efficiency/Size Ratio [cite: 23] |
| **SmolLM3-3B** | 3B | Strong | Competitive | - | Generalist + Reasoning [cite: 37] |
| **Gemma 3 270M** | 0.27B | Lower | - | - | Task-Specific Fine-Tuning [cite: 10] |

### 6.2 Trade-offs
*   **DeepSeek-R1-Distill**: Best for pure reasoning accuracy. However, the 1.5B model is weak in coding and general knowledge due to its small size and narrow focus [cite: 3, 12].
*   **Phi-4-mini**: Offers a more balanced profile with strong coding and reasoning, but is larger (3.8B) than the DeepSeek 1.5B option. The "Flash Reasoning" variant offers a unique speed advantage [cite: 17].
*   **Qwen 3 0.6B**: The ultimate edge model. While it may not match the absolute accuracy of the 7B models, its ability to run "thinking" processes on extremely constrained hardware (IoT, low-end mobile) is unique [cite: 8].

---

## 7. Methodologies for Fine-Tuning Reasoning

### 7.1 Knowledge Distillation
This is the dominant method for SLMs in 2025.
*   **Process**: A teacher model (e.g., DeepSeek-R1, GPT-4) generates synthetic samples consisting of `(Question, Chain-of-Thought, Answer)`.
*   **Filtering**: The data is rigorously filtered. For example, DeepSeek uses rejection sampling (keeping only correct answers) and readability filters [cite: 38].
*   **Application**: Used by DeepSeek-R1-Distill, Phi-4, and Qwen 3.

### 7.2 Group Relative Policy Optimization (GRPO)
GRPO is emerging as the preferred RL technique for fine-tuning reasoning in open-source models.
*   **Mechanism**: It generates a group of outputs for a single prompt and calculates the "advantage" of each output relative to the group average.
*   **Benefit**: Eliminates the need for a value function (critic) model, saving significant memory (VRAM), which is critical when training on consumer hardware [cite: 28, 39, 40].
*   **Adoption**: Used by DeepSeek (for R1 training) and widely adopted by the community for fine-tuning Llama and Gemma models [cite: 28, 29].

### 7.3 Hybrid Architectures
*   **Thinking Modes**: Integrating a toggle for reasoning allows models to be versatile. This is seen in Qwen 3 and SmolLM3.
*   **SambaY (Phi-4-Flash)**: Combining SSMs (Mamba) with Attention allows for infinite context processing with linear complexity, addressing the latency bottleneck of CoT generation [cite: 17].

---

## 8. Conclusion
The era of "bigger is better" is being challenged by "smarter is better." The research indicates that **reasoning is not solely an emergent property of scale**, but a learnable behavior that can be distilled into models as small as 600 million parameters.

For developers and researchers:
*   **For maximum reasoning accuracy at <2GB VRAM**: Use **DeepSeek-R1-Distill-Qwen-1.5B**.
*   **For a balanced edge assistant (Code + Math)**: Use **Phi-4-mini** (3.8B).
*   **For extreme edge/IoT**: Use **Qwen3-0.6B** or fine-tune **Gemma 3 270M**.
*   **For custom reasoning training**: Utilize **GRPO** with **Gemma 3** or **Llama 3.2** bases.

---

## References

### Academic Papers
*   **DeepSeek-R1**: *DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning*. arXiv:2501.12948. [cite: 41, 42]
*   **DeepSeekMath**: *DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models*. arXiv:2402.03300. [cite: 39, 40, 43]
*   **Phi-4 Technical Report**: *Phi-4 Technical Report*. arXiv:2412.08905. [cite: 18, 19, 44, 45]
*   **Phi-4-Mini**: *Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs*. arXiv:2503.01743. [cite: 20, 21, 46]
*   **Qwen3 Technical Report**: *Qwen3 Technical Report*. arXiv:2505.09388. [cite: 7, 8, 42, 47]
*   **Gemma 3 Technical Report**: *Gemma 3 Technical Report*. arXiv:2503.19786. [cite: 27, 48, 49, 50, 51]
*   **Math Scaling**: *A Glimpse of the Mathematical Capabilities of Small Language Models*. arXiv:2409.XXXXX (OpenReview id: fL8sds4naU). [cite: 52]

### Code Repositories
*   **DeepSeek-Math**: https://github.com/deepseek-ai/DeepSeek-Math [cite: 39, 40]
*   **DeepSeek-R1**: https://github.com/deepseek-ai/DeepSeek-R1 [cite: 53]
*   **Qwen 3**: https://github.com/QwenLM/Qwen3 [cite: 7, 54]
*   **Gemma 3 Reasoning**: https://github.com/himudigonda/reasoning-gemma3 [cite: 33]
*   **Unsloth (GRPO Training)**: https://github.com/unslothai/unsloth [cite: 28]
*   **React Native ExecuTorch Qwen**: https://huggingface.co/software-mansion/react-native-executorch-qwen-3 [cite: 55]

### Websites & Documentation
*   **DeepSeek-R1-Distill-Qwen-1.5B (Hugging Face)**: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B [cite: 53, 56]
*   **Phi-4-mini-instruct (Hugging Face)**: https://huggingface.co/microsoft/Phi-4-mini-instruct [cite: 14]
*   **Phi-4-mini-reasoning (Hugging Face)**: https://huggingface.co/microsoft/Phi-4-mini-reasoning [cite: 5]
*   **Phi-4-mini-flash-reasoning (Hugging Face)**: https://huggingface.co/microsoft/Phi-4-mini-flash-reasoning [cite: 4]
*   **Qwen3-0.6B (Hugging Face)**: https://huggingface.co/Qwen/Qwen3-0.6B [cite: 8]
*   **Gemma 3 270M (Hugging Face)**: https://huggingface.co/google/gemma-3-270m [cite: 57]
*   **SmolLM3-3B (Hugging Face)**: https://huggingface.co/HuggingFaceTB/SmolLM3-3B [cite: 35]
*   **Ollama Library (DeepSeek-R1)**: https://ollama.com/library/deepseek-r1 [cite: 56]
*   **Ollama Library (Gemma 3)**: https://ollama.com/library/gemma3:270m [cite: 58]

### Blog Posts & Articles
*   **ColorWhistle (SLMs 2025)**: https://colorwhistle.com/small-language-models/ [cite: 59]
*   **DataCamp (Top SLMs)**: https://www.datacamp.com/blog/top-small-language-models [cite: 60]
*   **KDnuggets (Top 7 SLMs)**: https://www.kdnuggets.com/top-7-small-language-models [cite: 22]
*   **GOpenAI (Unlocking Reasoning)**: https://blog.gopenai.com/unlocking-reasoning-on-small-language-models-f48ce438f1fa [cite: 61]
*   **Coding Nexus (Train Math LLM)**: https://medium.com/coding-nexus/how-to-train-a-small-llm-to-solve-math-problems-5cea85773961 [cite: 28]
*   **Google Developers (Gemma 3 270M)**: https://developers.googleblog.com/en/introducing-gemma-3-270m/ [cite: 10]
*   **MarkTechPost (SmolLM3)**: https://www.marktechpost.com/2025/07/08/hugging-face-releases-smollm3-a-3b-long-context-multilingual-reasoning-model/ [cite: 34]
*   **Microsoft Tech Community (Phi-4)**: https://techcommunity.microsoft.com/blog/educatordeveloperblog/welcome-to-the-new-phi-4-models---microsoft-phi-4-mini--phi-4-multimodal/4386037 [cite: 6]

### Videos
*   **Fine-tuning LLMs for Reasoning (UBIAI)**: https://www.youtube.com/watch?v=6tkV4XJ91Bg [cite: 62]
*   **DeepSeek R1 Distill Explained**: https://www.youtube.com/watch?v=KhY9XK1jGCQ [cite: 53]
*   **SmolLM3 Explained**: https://www.youtube.com/watch?v=5rUzDBOA8qE [cite: 36]
*   **Gemma 3 270M Review**: https://www.youtube.com/watch?v=Sp4qE3jDi0M [cite: 63]
*   **Gemma 3 GRPO Tutorial**: https://www.youtube.com/watch?v=2eMDwW1ftz0 [cite: 32]

**Sources:**
1. [huggingface.co](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF-N4LahLngDd3DtKPlINryToDugsJ7WYi4iHmdXUVfa5kCKmi0j8cQRf-oTcPyaS9jnwh5FhHyd-guNUNSp98hAW6gQzfDf_JCCAj7FQiRqNIk_IJk5HCSJBOl1SzY423Q31aLFaaCCAwHMbY2tl9V8Jllzw==)
2. [datacamp.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE9RxU_QLZYhJa57x0jdJqcX4LQ1H3irQvkcLbF2vhYo1kcdVS42S8SIAQZW7UQvFrjVjWaOxfCWCOz-QubeQOOJv-Ha4LeO8pbc6I6YILPC6XlTJ0ctfRg_xaqtpiTSi0yiD2KDUN_Yn-d4vhDcktAQ7mmUDhzP9aHUrvxawgh)
3. [medium.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGEmRxu8BvHjlfvp7C1pfRKcEX3GRe6cFfa9U5XmkQBlYNKS8COH9jeoWIRuT2Q3s6l3lmenDngw1BDbWkcHwlENwASB9au-xGcOSM1R6yvsVh3--xOA-qpcDSDa1laSxDxAuGAH_a2zFtfFMROi9MbQMAy6NUQV7JjP0EfX2XDJXZZcFNTM9hMHo1CoQ8ij6D7kL-cOE1W8PewTEVjJCBQPojWJnlZKhM=)
4. [huggingface.co](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGGspIBzKeTvzxSru2k6Ty9Uj9m11JJGb7mOiZevnJt1U5mmBXe0Vmk_1lKj6pGH6qVYeAhLa5_lyhE2ZdnJiURLNfO-oKKGmOj0QxG3zAuV_71XLyeJ8Vk95blDHP08GXo-KrBA1uP_yRcfCi-BiL1-w==)
5. [huggingface.co](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGkPZucWlRGx3-1KuSLW1dokN7T42sMPg2UwHvHEn9aRyb75r2Kfs7yhq5cbq52WRVslbx06vKPoZWIwniwsx_gFj3kKL2oHSHx5US_cmKqby5wzRwzvTDZIrtCN5E50s3KyMI7yVk_LTWSKA==)
6. [microsoft.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGCix-LJzJS08RXPVGYgtTuDvoq4R8uxd6ZZDXRNCTHBEB5Ls0683UUAUgpjhYrk_eKb6GewcA3ImXywAB4FkfOdEmFFp-a1RrV6DcrFV83cRUhtD7K_iG6pnNvKDbLL5VCYTRoGE6XMf0Nw9K7c2CkyxJD4KHEMLBfZ6KZpfG8LT_8FR4k6m3PXXE7Uu6MHirKuVUD_v0tVQFPQxC_jSNyR7dlf0-qFZdlz3rgQfc0X-OQNoKzmAS9mFZs-vq6oDSNI6qQuZo=)
7. [arxiv.org](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFOiOG4snK45bVTpHNQ4CbEUazjy_KqoqNNfLnItg-7wsi1u_NH44oyUtZjIsF9AYPTwRoX-XZINRlomgOzOk86hlw9enwqa_-qDOYKdLjDyx7-DhBAKg==)
8. [huggingface.co](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEVSFl41J3PenQA0PGUlCkmbXvSIRJpVas6wnYOKQeG3NDe3p4q8OkOlJRSuydk5AXknlfXZraMkluWCG2xbQTQwLirsYMklCRf6RORjww7EYcPLx6Q5_hEKjCp1A==)
9. [smollm3.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG-ZqUiAec3Qy5ujbh87dmErQDMRLgKUiCTCFsYDfHW1octlu59BLgMuxDq-8ZbXGIjWHjK-ArAtYbb7BDJLmc9_88CGHRlKv2qyA==)
10. [googleblog.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHfix4R-qNjaxk4unHaQf_dSCgqbLuJTZw_dxs5yNiIU6Iuuvf8oYxvgIsR0H5OfcEbgVoNtJB9lTmuJ4XbUtA8fyDrmCBpX-9cQF8VXzYRA1e732VVjLGQtA_hJ7Qy6VdeSFBkljGxMlTs9itEfQBFp-o6RA==)
11. [huggingface.co](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGqbKTIj798NoIH2D2HDVXuBppmn6EAD_WElrnggJOa-T12LkH9dI23_pbVSRfP-1Vmd8xkmpMuvySMS1rC3Z9Iv2g11ld3ZjZc2WpmRifrwkIEvQCspeCj0wsPmF-oab2ijoUnZxL3tFpjBHCQVHgQjw==)
12. [bentoml.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGxwe8W5aRgaCWiOZ6s7PFEYDShVPImbaBtQ5T1XuySoLXYMNcRrIHNVRNFcljnZmjKIJzAvXlsv-pLEQ7fI4AeYKHFUC732o6MdjtsNXX--x5_KgowSBq27bE8M-fMk4ceURhRugJHxOA_dLk0vLHxReri283pFOM3TZLEgDJ7KGQZ7PFTHfVaDsnWnWMHaMbG)
13. [huggingface.co](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEW8ijGwnqk6gKlFndCUidlR5rnh3f9JD227dSOIC7rYMxiJH955mDO2XbJBU-tVdKJVchzAcdKoJKTSlygk34_Cd50dLGpVsJ_HFWpEuWPWrPp6hVv89TWeqh-xPrZgfjOeb79EiEzcTpi5gmxlzVA67fTsUC9WWjrIYoU4kuNt9_dNJKK)
14. [huggingface.co](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHP9V9qhQWRgWstDmG72rgzhNQfUVQy8QArr8kxC_UEZl0njUM3gHw75vfFQ6xoeb1aCaBKWOXXIIwf08YFXPfHBpQHwC-4MOalcsTRaSm35vMgGsRyQiADolDFgRG_kn-xJrd8X5RMi0jG)
15. [ollama.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEMgLIsNSUjvcKv6nqGsJ_ETl7f0GfJ1GgsO0C_3m4qilmaew8XjxbLf6VAgEmciXoSdEXQm6jx3bXi2iy6QR58YHTxN97rez0R2QtBGm7lc579Dw195PDuHaI3s3Ms)
16. [lmstudio.ai](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFUCXNLQy1BwGHUIvjTy0hF_rLjpevR0Hm5oXcsM-_yujD6OT70EeKaTmlDf047UzHKvqBLIOBG94ybTCZm9mVWBPBxd4SYMXQUpeudQG8BARDiL8EFodZJW_MO3V4O2yw=)
17. [medium.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHfx9ArKZufpRPfkEHfJC-v1GzZ7tqHtCnoCsvstbC13I_P38fCWOpTzlgtVpE85U64hysA2_K8LsN1-whUhSrwakLcPvvxYrmxWPY_zehXOCPWmIpwL_M7ZTYF2SypYfeB8ylKHLEqxirdw4PdqtHXePAPW1bEmSqukJb2y0RLSWjAR1evfy6zAQw7WsrVOINvdiRL)
18. [researchgate.net](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE8pPH1ZVnyxPaAJF01X8rVkaduKQZA9zY9xf7PAXaiXbQXWRSSmok66fijqNy0L6y-uEMbnQUK7m30sS6M0EmCuiajDZje6ULV7AVelCZZ8hiNIsDFfoBBPVsFK0u_l1d99px7y3nUrtiGDvC8cLsZkLvj27e0jCRVpGl_U4B0)
19. [medium.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFxE6o-Gl2E-MgOxB2Zuvp7d7FOy120uF5lhqtOdpZe1XsW8vMtVTSO77JiDBuYzHsrjMMGzRIOBShea20MmOGXpiurIh7g-GuZZctDqiAdKoMXZx644d957aFp8-4X-6a2G49rZsV6P6whj25OzkGyiIxzZrHGPE-t)
20. [arxiv.org](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFGLOEjdO5HnrJa9LyZu1QfMLqq0EnSclw5Ko5GTKL-dpErmywjOp6JUmP4PzeyrYifXmCtYa-prXf4eMjMtnrzJ_RJbI76RwGeW_0FSMgxdq1fT5mfog==)
21. [medium.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFmYgrjGGbSO75CWTO0uxOgyeHs5RsW3C2lrpj68p7tbmrQaC-cUQuSdRww727jLwnQ_IORyMNpRWr82uop-539b0ce0-Bwmjd50B0jKbqvV0uvYvJsTHZ1jU_6XqHyWo-YG9nPdlo4HnuWEqfj_OrCMNAwUt-ra8J2WBaBuWG4tkr-yaPLYRj4zzCUfqO1zg==)
22. [kdnuggets.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKPOB0qKRH5h7ddU6ZZGRAZaeEwfQKKTOK8U9Vz3zomokidChMybxAZAunwtuyhAVvxqzFasR5dxezLA1XOZpYnR79pSoJPDFXVHZXDmgppyev35mqTS5tnE8CTbWzum5PE09qeEqqYk38JQ==)
23. [openlaboratory.ai](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHG5QGMN958EPUh6ZkWxadY8KuSZ9DlePVO6NWlbKWsLjUY3nU4XwNkwLGHrCFcF8sGItNSIbWB8WJLpOQJR3ALIoebrICKQKCa6Gg_iFr3kSLJS79zmWfqWJ_BKzThmR4x)
24. [opencsg.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHNATGd2Nubuesh3wndvaO8Zzb7S-ZHv0qRfLqZ-CEkZSoBEKoq0ddvr1NkIcGeit_7ghBmdYBlJUjWuzxfZjknBMW81vVlfbvcM4m_OObQhVr2PYhKbdflhO1UpbxABHMsHfZoow==)
25. [medium.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFNsTwXjcD25vhYV2msYIk6lit0DaJR1DuxB7S1PGDsMvrVX5i6TjT784ItDH4erFDikfOEGTdB4HJBtQ1EUcX6lo6dX4_yZGH6-uz7xV6m142Gr1FHrs7XL42p1sEdH1bqhOny6nnw9pWPWHci_y3nwNpHivPjeylpFB8vxKWQKNTai4dD1xsMzvh_ZW2yPgJyLOyu796cW9m6)
26. [apxml.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEnETolf9XukN0OjeF_LtNyQ0c034K39KhSsksa45tKnqSwQoJQ-KRX90U0F4fRBjF4SKws_QVJYYSowb4J9Nsc4llkC5p565Y8r6VjmCmwjULKi0c_xDsBQQ==)
27. [ycombinator.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFNb9mYg-XZMZOwc_wWWwD8YbnVilj5DABFbFBOdTRPxmuJLN5UxL4rsm9jBNq7S0mQu3QQQJA1tZreomBLkMnvlPBh6-hK_4EkhSUPJ1qv30DilSBkYj4jcw9_K7YtthORgvI=)
28. [medium.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEOAuNEOT3baZprrdF0n7JakdyDiF7Bgamg58Be8xVwRA6PlsrLoeCSLolDB6hptkorN94IawtLKgwzGed_wyxRsfUUIMOr7lm6jiRVFavs8sVl5b0kiSraB0c5bJZVNiOXLHG3g6HaFf7vd4v-TsTlJekSCJYO6uY0oZGQYn5RqezzzspBbyembZ7S7JvCEyUG5w==)
29. [medium.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtvPdLsa32xoDq3KAp8vhMP1bY-zowvBQRLxUsg8LXavI31t0OlxQrmSVJkE5sedIjzqI2XwqsLXpkG6cv_Qi5BUB0B_aUS7hzeN6DZ-yr7b31YzwplMrdr22YMYalIDCUr2qU6vnXkpkd8xULMmlcczLrT7_0Qyx1W3papsrhnr1TTiu5jX-bEK5n6XGgT67SP01o4q1SrE9-vXVjov60EkQyMLI=)
30. [substack.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFtxm4FOHpYsydB8L4X1jb3YUKfmiBEpODFGGoW7Ewk9mW3R4ENrEOYgO4Ct5UR3bklqOWUwymgYBZGDm_OTJPrrUnLIO7EbKzbyWruv24z7iqX5BjIiJX7Lkm6CnFl0bWpcaBpKLx8CBbl3YToiRyT4so9GKc421E=)
31. [plainenglish.io](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHP0NpO0TyUC1Stxrzkmc79BcggHs82tG9v9N1bg82m26XA_70WBX-pRKkY1Q6AW88SiL_dXhCcKFp2m19BjkOAwjU7ASIAvr6FT53bNg6a7q3Ust9xF5R9PrPjn7KAwGu97A_69tj3mn5iOLa27asNas2UaajfnhMW50Ot3NLZ8sD51_Qsn_DhHSsnbTWQ2sWSa_td4UlA_HVC2aO-Rly2qvM2ahjFg3QFneC_pue7qlSg4wV9IJZll5U=)
32. [youtube.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGwqbegxawbg05ONqxZnp_uzKwV7NliFaJPbjELiZ27Ts56Ww_H3C0KG3azXEJPR_V_vPAYuEm5DitRiUA4H7rkzFOApB5JP8vJyXVj2M0JD6MeC0e8DEKYW88ZUUpblz1q)
33. [github.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEtRapsMmqIfqk0R6WT3fnp77w4P2DmSgwxeOlS6ylxA2EQMzu-uZ9UZybp7vL81fR1WFr_eXNgaJLKLYYIFA7lii_2kcXGx6yPb2H9_duTFpCEqpXVjEweQkN3hdPJJCWPfqQxiA==)
34. [marktechpost.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQENgs4Vt19amvhwBuw3XThPvK6Mi_vL-ICHr_BZjZlTDRIfYpUsZLm4aGOzwAys08MmKmNy-LLFYGiPXNwMJdaO0WBiv_ulEIotzw5RKsPXBBmbgWxsB2roRuxid4K29AembNWqYEMLLTyTy5CBh6KUyMegzlg5sQOIAUdWpwz6weyDtnjAm260EYmerYkFwv68OUo-nk1LHhjUYX0HcuATD4WUS1-2jzgwvEs=)
35. [huggingface.co](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGj9ghrkih5Xuwb4OFkjgHrpkqH6EMclDoDgt9WxTW8BVWRRySy0_An8NJf-BpGB8C22uZHBpzeKHHD6Y8SEXzd0_rUaaWv1KPWE2ZAQigw8rgr6JFiiIr-FV5VIVlJpmiMxthEWA==)
36. [youtube.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHTMtHRNRJokiZW4BCPRB_e3f7r6ZY7SDVU9b6wYGbeu-b1csP38tNuIBR_XnNC3CmCcVp38j2GVrvYuIhuIPDe6Nv7G0uhAgPcv4NgEmBCGi1EhlrtSK86PKbtiDX6t-36)
37. [medium.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGhu7qLdbi87tarPdmAu4rzGi2ttlFXXdfhNX0nopcx6w8dXYeQM4uNnD_TjtKPYtl1G62G6-mBLrCgCA1JeQLs4qwtPdAPqxkwcKEx3XLOQdGwUIWo1qyZjb6uwn1pICMw1U6U43KT-AJwUYVjRrRz4_Q4TNf_k0EyXHxeR7T4XAKCK_E_J8SGWVn4hTOlkFc2Ei8deHGL0TV7Fnl2y5I_Lfxyj9NcAw2XrN6AL2LhpwBIwfrKaBdpsA0u03JmBzp3GGpfzdHMSuL_tY90)
38. [wikipedia.org](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEHdtNNeht3kKeubXCi_kkhqg4-o8YBnV9amUzcv2d08jbFk5jbSDHNlsyN1j82cIlOzCaLOm8ynw-xcbwrlst6hv-EZtdS7_EOLZAKpwkxpXBpO2JYDohTljK5kg==)
39. [reddit.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG0OU9QHHfTZf528cTWOvI5P-sU1SUOPbo-qYlC2pnYVZmf3d0Wngt40jm6fSBQjnwgUpx3RsCWn4pFF4kQhFjChXXl4TB6VjKRXVp6IXDkoR6U7VOgEl583-Xi_gIOZvFHgPNqQl4W9lp4nHo89xdnoTWlC35yNnpBGtHOXEUKxj9bYj5hy8SLe5_1cLr8XT70aW4uGgi68xLY)
40. [arxiv.org](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFoqRcxlffK3qUrDqHpmuRfxCkXlvles4VQV3M2g72YmVrYx-yFlVJTQSa-HqK8kzo2GKlRUdmDmgZJCFaW8aC_26cYBQczBWkCw5j-jSrTFfiiDueHNmk5uQ==)
41. [arxiv.org](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGq57gj5fXymSoVLaTHC-tJCKhabFDOWf0D-RDJFd8uuPFI7svaiupIAWvv31pbnQAzH1eDnYTs54ESyAOsTTF-YfQ4rVE-eDHaMsn-r53HTG5NE26DKA==)
42. [arxiv.org](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE-8vHuq4blxHu_mSjDKVpsu41Zf0-Btumk2ZPDRM-SjZGR-oxHkvgDzGoTEHneVOo1dDWS1o8U6pKjfqpXmn_dCk754vNW7bxWuY4H3RiIeYYS80sFpKsq8w==)
43. [arxiv.org](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEIjhBM9qjEhMiZuRTITxXubU5pW2LE_uGY1wF6nTUZLt89r_ot4D9Qv-kVHlRpOdFK1JtkXzuZ70IsffqGDi5gu2SukszJL6udB9NbT6dfox2ejBi6ng==)
44. [microsoft.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFS2Z9EezeexGx9rgHdR4GHsA4gtm-CgIRDDhQ627xehRdjPr14H5ZDgc-2G6X9pZmcLuQG_d2AwdASs89xw0UCx6VtvRakjmMZFO1uPmreJJYJN71iq-WE40_p94UnmaYm0adb5d2GN_eqm7IBFB4M21lWh-8ALlNXTTlwyif7awMBzekmQXsP1F8=)
45. [microsoft.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGJZnZoDFaMhOyZYKzs4syE67akBm19j2nnG5Lz3iQ2W64DqiJh_cJbvy20tOMLFjE3QVbMVGuvUWoxsskpmurQIydA-KE_2mnsjJp0gvUQEJJ8grDHV-K0auQXcfFu-LMF5v1DGypehcLupjBMPaXOFKup-nd1fc5YnW0riuJ3ZI6c)
46. [researchgate.net](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHv6Ny_UTwdQUmGqYd6eFdcqGwI9rYnAxrxAtAWnGvaoMNFgXmtWTB4lkuFi0BsrarnNSG-WQp5brlCdY59JgyQqruTsqGbvrZn9riEFzIxhFFlBHy-1u1__grmTN0tuIR7DDOld8-Yg-VvbFEF4OVKNxxnbg7j46pKzR_G62JAA-OqJhDRncEdIwbtfEpqnpSUpkqcoVQT6yuK44IaIwSiMM0k-nLqDSgN8aLZ7PDNX1I74ZoLaiEE7-nguETnpGj9d5fKYViRnW2z)
47. [huggingface.co](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHSZ1AhL0fMZAk2DeumuaYfyCGQH_Tg_7AnYPwl60wP_1KpPs0ACyfgbTYQBQfwYJKoV7_5qj5epFP3PWH4WBptHz8OAEKK5kKYt6ZKJ-Hv6WFvmQccTIqGpJNso5fE)
48. [arxiv.org](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHM_LqzVNgchta3A49syZte-9uXhFfgyYhdoTdHKSVXZatlNZ9M6OHcNtHPO7z1WIpgooGTTqSN1brhThYA-RN8WPV815dvt296kGPWWGj6iIdxeqVbrQ==)
49. [hyper.ai](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8c0OPwOLakMrav7wzF9CBAlCbfh8sTfq6yi7b-QUlV0Ht8IrZJT0dx-P9LTB2L0Aa9NU9IV_3ADdq4GL-6ckfykeJosOGP-6FG9oi67lsI7LXZFltdZ4lmw==)
50. [ycombinator.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHYmF-gtZYtyoO4KvQdKjxEMkxbZWKv8sQPdswCf7qIuC8MxrWPQbhxVcmxpVkwqJ_LhdDaeScBVImv21SbHyqH9ID3UrPgBpbNih75MtZu4QWUkEsNI4fe2rDmNQMxI9BFfxI=)
51. [huggingface.co](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHUoGGWdTcoanxFQ4IpX1UJI4A7rHjf9XEMx1pDpl2zTpSbWAPbL_kZe66OV35-h4Na_Y8aH6nciGtrRcCoWmk_W0jtZOXtYa-iwPGE4E-wBY7Ma5CpXzt1s_9tlM7u)
52. [openreview.net](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE5UiezWQ4CEIJzvebU2VK40ZL6wrmK2SPz8Uo03ZLE0z5iTx-7QjQwcN3vRQw_Fy5fFKpXiAG5QtbmZJ3020LlJSV3GHN0mYQfvh6eSWA6vBIHXQnNMnigAZ7Z6Vu2trU=)
53. [youtube.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFGOC-3JrXZiQzbux_bsXQJCFZjdF3uiS_tI0WDDRfKmnC5mu-Bb20YtX6w8I_VBQ3PwBQED9YpXNX05sFclCzXeEwXYwEB9FSOmwBILICsVJii0DWmgq7KngpqYhs5jYiw)
54. [github.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEqe_jGSYe9obZFM1MoJquz6COvuaDr-6G187iw2zruMXhkDCcoYbPhAEwHPoszay-qxCeYLZzy01-l5EBVI0F96rE4GSoutFcf7R--l-WTjciGcGgS)
55. [huggingface.co](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGGXrZ-Xrxt4BTJ9DXAP1OF3xXvRI9X05b383UlaatkuahLwyQuSuOZnKSnKTmkiWTNrIfoK-WZI5Bhr5a4qe8qV88rSNsv54LnwfY09VQwyl7prdau1p27qVyaQI5Vhdl_AKwdI4DDH7G-iYPkZcSopFOxbANvqW4t6fHY)
56. [ollama.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEUadB66qM4rkBdB4sTx36C4CZ82ihP9-mwmk_MbfiA7lWRjDTRCHh0sxilfnX3ui7RFY9CacxSC4UMhya0qVkWK4DQvJyXiaFEG8rpCpJL8iKHgEEQ8BieeMbnzQ==)
57. [huggingface.co](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFO7JOix6OuqDj5Syy5oBC37UD8pa1VxIQP4y7IjGpZoJ-Ee9dvewx-hInGL61-JfVDQAGteLlhudELhRl6vVOOb0-hjn_7mMAffzMy0JOe9L9Uxahsx9uFTTw4Y3lWyx8=)
58. [ollama.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEt4s7HsQ4ZFnKr4s3xARZrjXpH89Iog6RuzQYj6mJqM6ytNs7ZRVgSobctf52QFvEnwi3R-7j3DN9aX5U7x92k9-6nvDN3M97gdCflLwtBhyUM2qooiq1qZ-1J_Q==)
59. [colorwhistle.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF111qcm2FcnSmzwWAtp7Jwv4D__6DGNE4kJWOIJgQ08rnetPosePwApmaSMJofvweisSb3ehnTVoCajEA61qg7yhmSqhretuUQQtamXEjJF6bGnHnXawiRjL3I3NGORH9LHHuT7w==)
60. [datacamp.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEiQtwRXalrT6ZO98Vq10C403-Us3r5ndTHuNABeU7g7gwco_ppM_FyTH6sBxKE_VWz_4F65j_qed8zKXpd-UtlkHai9vzL9h6RrRkltq_4FSpKrFS3y2Fh4WL40YF0sqYPGcoBu7xWr5IV5GL_)
61. [gopenai.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGOMLr8iDYv9tQ-3K0CpDv4szIM2IVlC9_EHOXeIlVs9qgteYBZTb-YgGzr-R5-WasBjEu4GPWsAqn5ggNSbQnlq6wN6sNxP2pryfaRg-k4hGm3ahU2AT9ZF9zC3awM5L1NUYfj_RHlvooUWd63fN7ouLsvv0jzKN-j3A-WEKG_5Iy_yjew-eIh)
62. [youtube.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKPQU4XxOhKmd73n5Io8qH0YXBi6YpP5jRsez_Pp6Fj1OKMi-CMOTZENM6XWq5nR5mbiVgPmZJcn0InSHTyoA48TBbP_Qdf5wMj2H4BiLg0wieI7mR9muMBQGxpQA0eTb_)
63. [youtube.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuulv3XzArkk6-asKQAAxMd4oodc5TiZQlz5kznbKO1otAZkjLrnEi3w0zxTjimrkophPukbb7qUyyAJ8dONIHAsCjdECzyf_IlO-X9xFqu1uhSxzfZCkH-psMyJ_zqpZq)
